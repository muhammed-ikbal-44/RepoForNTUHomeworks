{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b82f8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: torch in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: transformers in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: datasets in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: tiktoken in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: wandb in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (0.22.2)\n",
      "Requirement already satisfied: tqdm in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (2.12.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (2.41.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pydantic<3->wandb) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 5\n",
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\İkbal Özbey\\AppData\\Local\\Temp\\ipykernel_17812\\1173437289.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ede0ac5-97e7-4edf-ac1d-a18e0bfd9502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_neg_pairs.json created with 20000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def generate_example():\n",
    "    # Sizin orijinal kodunuz temel alındı.\n",
    "    a = random.randint(2, 100)\n",
    "    b = random.randint(2, 100)\n",
    "    op = random.choice([\"+\", \"-\", \"*\", \"/\"])\n",
    "\n",
    "    # Doğru sonucu hesapla\n",
    "    if op == \"+\":\n",
    "        res = a + b\n",
    "    elif op == \"-\":\n",
    "        res = a - b\n",
    "    elif op == \"*\":\n",
    "        # 2. DÜZENLEME: Çarpım sonuçlarının çok büyük olmasını engellemek için.\n",
    "        # Sayıları daha küçük bir aralıkla yeniden seçiyoruz ki sonuçlar yönetilebilir olsun.\n",
    "        a = random.randint(2, 35)\n",
    "        b = random.randint(2, 35)\n",
    "        res = a * b\n",
    "    else: # op == '/'\n",
    "        # Orijinal ondalıklı bölme mantığınız korunuyor.\n",
    "        if b == 0:\n",
    "            b = 1\n",
    "        res = round(a / b, 2)\n",
    "\n",
    "    chose_value=random.random()\n",
    "    # Orijinal basit cebir ve aritmetik ayrımınız korunuyor.\n",
    "    if  chose_value < 0.33:\n",
    "        question = f\"x{op}{b}={res}, x=?\"\n",
    "        \n",
    "        if op == \"+\":\n",
    "            reasoning = f\"{res}-{b}\"\n",
    "            ans = res - b\n",
    "        elif op == \"-\":\n",
    "            reasoning = f\"{res}+{b}\"\n",
    "            ans = res + b\n",
    "        elif op == \"*\":\n",
    "            reasoning = f\"{res}/{b}\"\n",
    "            ans = round(res / b, 2)\n",
    "        else:\n",
    "            reasoning = f\"{res}*{b}\"\n",
    "            ans = round(res * b, 2)\n",
    "\n",
    "        pos = f\"{question} The answer is {ans} because {reasoning} equals {ans}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "    elif chose_value > 0.33 and chose_value < 0.66:\n",
    "\n",
    "        question = f\"{b}{op}x={res}, x=?\"\n",
    "        \n",
    "        if op == \"+\":\n",
    "            reasoning = f\"{res}-{b}\"\n",
    "            ans = res - b\n",
    "        elif op == \"-\":\n",
    "            reasoning = f\"{b}-{res}\"\n",
    "            ans = b - res\n",
    "        elif op == \"*\":\n",
    "            reasoning = f\"{res}/{b}\"\n",
    "            ans = round(res / b, 2)\n",
    "        else:\n",
    "            reasoning = f\"{b}/{res}\"\n",
    "            ans = round(b/res, 2)\n",
    "\n",
    "        pos = f\"{question} The answer is {ans} because {reasoning} equals {ans}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "\n",
    "        \n",
    "    else:\n",
    "        question = f\"{a}{op}{b}=?\"\n",
    "        pos = f\"{question} The answer is {res} because {a}{op}{b} equals {res}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "# --- Kodun geri kalanı aynı ---\n",
    "examples = [generate_example() for _ in range(100000)]\n",
    "\n",
    "with open(\"pos_neg_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"pos_neg_pairs.json created with\", len(examples))\"\"\"\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def generate_example():\n",
    "    # Sadece tam sayılarla çalışacak temiz bir yapı kuruyoruz.\n",
    "    op = random.choice([\"+\", \"-\", \"*\", \"/\"])\n",
    "    \n",
    "    # 2. Operatöre göre a, b ve res'i hatasız bir şekilde tanımlıyoruz\n",
    "    if op == \"+\":\n",
    "        # a + b = res\n",
    "        a = random.randint(2, 100)\n",
    "        b = random.randint(2, 100)\n",
    "        res = a + b\n",
    "    elif op == \"-\":\n",
    "        # a - b = res\n",
    "        a = random.randint(50, 150) # Sonucun negatif çıkabilmesi için a'yı b'den küçük seçebiliriz\n",
    "        b = random.randint(2, 100)\n",
    "        res = a - b\n",
    "    elif op == \"*\":\n",
    "        # a * b = res\n",
    "        # Modelin zorlanmaması için sayıları küçük tutuyoruz\n",
    "        a = random.randint(2, 35)\n",
    "        b = random.randint(2, 35)\n",
    "        if b == 0: b = 1\n",
    "        res = a * b\n",
    "    else: # op == \"/\"\n",
    "        # a / b = res (Tam sayı bölmesi)\n",
    "        # Önce cevabı (res) ve böleni (b) seçip, sonra bölüneni (a) hesaplıyoruz.\n",
    "        # Bu, sonucun her zaman tam sayı olmasını garantiler.\n",
    "        b = random.randint(2, 50) \n",
    "        if b == 0: b = 1\n",
    "        res = random.randint(2, 50)\n",
    "        a = res * b # a, b'ye tam bölünecek şekilde ayarlandı\n",
    "\n",
    "    # Problem tipini seç (cebir veya aritmetik)\n",
    "    chose_value = random.random()\n",
    "    \n",
    "    # --- CEBİRSEL DENKLEMLER ---\n",
    "    if chose_value < 0.66:\n",
    "        format_type = random.randint(1, 2)\n",
    "        \n",
    "        if format_type == 1: # Format: x op b = res\n",
    "            question = f\"x{op}{b}={res}, x=?\"\n",
    "            if op == '+': reasoning = f\"{res}-{b}\"\n",
    "            if op == '-': reasoning = f\"{res}+{b}\"\n",
    "            if op == '*': reasoning = f\"{int(res/b)}\" if b != 0 else \"0\"\n",
    "            if op == '/': reasoning = f\"{res}*{b}\"\n",
    "            ans = a # Bu formatta doğru cevap 'a'\n",
    "            \n",
    "        elif format_type == 2: # Format: a op x = res\n",
    "            question = f\"{a}{op}x={res}, x=?\"\n",
    "            if op == '+': reasoning = f\"{res}-{a}\"\n",
    "            if op == '-': reasoning = f\"{a}-{res}\"\n",
    "            if op == '*': reasoning = f\"{int(res/a)}\" if a != 0 else \"0\"\n",
    "            if op == '/': reasoning = f\"{int(a/res)}\" if res != 0 else \"0\"\n",
    "            ans = b # Bu formatta doğru cevap 'b'\n",
    "        \n",
    "        pos = f\"{question} The answer is {int(ans)} because {reasoning} equals {int(ans)}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "\n",
    "    # --- BASİT ARİTMETİK ---\n",
    "    else:\n",
    "        question = f\"{a}{op}{b}=?\"\n",
    "        # Aritmetik için doğru cevap 'res'dir.\n",
    "        pos = f\"{question} The answer is {int(res)} because {a}{op}{b} equals {int(res)}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "# --- Kodun geri kalanı aynı ---\n",
    "# Temizlenmiş ve basitleştirilmiş veriyi oluştur\n",
    "examples = [generate_example() for _ in range(20000)]\n",
    "\n",
    "with open(\"pos_neg_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"pos_neg_pairs.json created with\", len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae92d0dc-2ea2-4f46-835a-78694721975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20000\n",
      "Example pair:\n",
      " {'negative': \"22+56=? Sorry, I don't know!\", 'positive': '22+56=? The answer is 78 because 22+56 equals 78.'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"pos_neg_pairs.json\", \"r\") as f:\n",
    "   lines = json.load(f)\n",
    "\n",
    "print(\"Total samples:\", len(lines))\n",
    "print(\"Example pair:\\n\", lines[0])\n",
    "\n",
    "# Clean dataset: remove unseen characters like '!'\n",
    "for p in lines:\n",
    "    p[\"positive\"] = p[\"positive\"].replace(\"!\", \"\")\n",
    "    p[\"negative\"] = p[\"negative\"].replace(\"!\", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b063009-cbbb-4d20-8fb3-2cdfd8190166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(),lr=base_lr,betas=(0.9, 0.95),weight_decay=1e-4,)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend to use the AdamW optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d4ebeb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.0279: : 312it [00:35,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.0252: : 312it [00:35,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.0240: : 312it [00:36,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.0229: : 312it [00:36,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.0224: : 312it [00:36,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "total_steps = len(lines) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # forward pass\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "\n",
    "        # correct DPO loss (from the assignment paper)\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - 0.1 * pos_logprob.mean()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_norm=1.0) # Gradient clipping\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Epoch {epoch+1} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eeb4935a-ad94-4c7e-84bf-06d7114d018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.1\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\İkbal Özbey\\AppData\\Local\\Temp\\ipykernel_17812\\3955561830.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 17+19=?\n",
      "A: 17+19=? The answer is 38 because 17+19 equals 38.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q: 3*17=?\n",
      "A: 3*17=? The answer is 41 because 3*17 equals 41.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q: 72/4=?\n",
      "A: 72/4=? The answer is 16 because 72/4 equals 16.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q: 72-x=34,x=?\n",
      "A: 72-x=34,x=? The answer is 58 because 72-34 equals 58.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q: x*11=44,x=?\n",
      "A: x*11=44,x=? The answer is 4 because 4 equals 4.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q: 3*17=?\n",
      "A: 3*17=? The answer is 41 because 3*17 equals 41.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q: 72/4=?\n",
      "A: 72/4=? The answer is 16 because 72/4 equals 16.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Q: 72-x=34,x=?\n",
      "A: 72-x=34,x=? The answer is 47 because 72-34 equals 47.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set:\n",
    "        prompt_ids = encode(prompt)\n",
    "        \n",
    "        ################################################\n",
    "        # Please complete the test code here!\n",
    "        #\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        #\n",
    "        ################################################\n",
    "\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n",
    "        \n",
    "        # --- DÜZELTİLMİŞ SATIR ---\n",
    "        # Sabit değerler (200, 0.8, 200) yerine değişkenler kullanıldı.\n",
    "        \n",
    "        \n",
    "        y = gpt.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # -------------------------\n",
    "        \n",
    "        output_text = decode(y[0].flatten().tolist())\n",
    "        \n",
    "        print(f\"Q: {prompt}\\nA: {output_text}\\n{'--'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478951c-1e12-4f3b-932a-892d1a1184c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpt2)",
   "language": "python",
   "name": "gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
